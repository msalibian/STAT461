---
title: "Bootstrap 1"
date: "`r format(Sys.Date())`"
output:
  html_document:
    theme: readable
---


<!-- output: github_document -->
<!-- --- -->

<!-- notedown --nomagic Boot1.Rmd > Boot1.ipynb -->


<!-- themes: https://bootswatch.com/3/ -->
<!-- readable, flatly, paper, journal, cosmo, lumen, sandstone,
    spacelab, yeti -->
<!-- output: html_document -->
<!-- output: -->
<!--   pdf_document: -->
<!--   latex_engine: xelatex -->

<!-- notedown --nomagic SpatialTemporal.Rmd > SpatialTemporal.ipynb -->


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## A few examples

In these notes we will use a few simple examples to illustrate 
how to use the bootstrap in practice to construct confidence intervals for
a  "parameter" of an underlying and unknown distribution. These
first examples are very simple and only meant to "fix ideas" about
how the use of bootstrap looks outside the blackboard. Furthermore, 
in these synthetic examples we actually know the true distribution 
that generated the sample (and thus,
we also know the value of the parameter of interest). This will 
allow us to assess
the level of success of this inferential approach. 

In these examples we will only use the simplest bootstrap 
percentile method to construct confidence intervals. 

### Example 1 - Estimate the square root of the expected value

Consider a sample $X_1$, \ldots, $X_n$ from an unknown distribution $F$.
We assume that the mean $\mu = E_F(X)$ exists and are interested in
building an approximate 95\% confidence interval for its square
root: $\theta = \sqrt{\mu}$. 

Here we first generate a sample of size $n = 30$. 
```{r ex1}
n <- 30
x <- rexp(n, rate=4)
```
Note that for these data, $\theta = 1/2$. A natural estimator 
for $\sqrt{\mu}$ is the square root of the sample mean
(e.g. this is the MLE of $\theta$):
```{r ex1.1}
( t0 <- sqrt(mean(x)) )
```
We will not estimate the distribution of $\sqrt{ \bar{X}_n}$
using the bootstrap. For this, we sample a large number 
$M = 5000$ of samples of size $n = 30$ from 
the empirical distribution $\hat{F}_n$. As we discussed 
in class, drawing a sample from the empirical distribution
is equivalent to drawing $n$ independent observations 
from the set $\left\{ X_1, \ldots, X_n \right\}$
with replacement. In `R` we can do this by simply
drawing a sample of the indices, i.e. we 
draw $n$ points (with replacement) from the set
$\left\{1, 2, \ldots, n \right\}$ and re-compute
the estimator with the corresponding points in the sample. 
Note that bootstrap samples may (and typically do) contain
repeated observations (even if the "original" sample does not). 

Here is one bootstrap sample, and its corresponding 
re-computed statistic:
```{r ex1.0}
bb <- sample.int(n, repl=TRUE)
sort(bb)
```
Here are the correponding observations
```{r ex1.01}
x[bb]
```
and the associated estimator
```{r ex1.02}
sqrt(mean(x[bb]))
```

We now do this $M = 5000$ times, and collect all the recomputed
estimators in a vector. Note that there are more efficient ways 
of doing these calculations, but here I am giving preference to
transparency over computational efficiency.
```{r ex1.2}
M <- 5000
rho <- vector('numeric', M)
for(j in 1:M) {
  bb <- sample.int(n, repl=TRUE)
  # bb <- sample(1:n, repl=TRUE)
  rho[j] <- sqrt(mean(x[bb]))
}
```
We can look at a histogram and boxplot of the recomputed
estimators. These are bootstrap estimators of the true
sampling distribution of $\hat{\theta}_n = \sqrt{ \bar{X}_n }$:
```{r ex1.21, fig.show="hold", out.width="50%"}
hist(rho, main=expression(rho))
boxplot(rho, main=expression(rho))
```

We now use the quantiles from the centered bootstrap 
distribution to build an approximate 95\% confidence
interval for $\theta = \sqrt{\mu}$:
```{r ex1.25}
lo0 <- t0 - quantile(rho - t0, .975)
up0 <- t0 - quantile(rho - t0, .025)
lo4 <- quantile(rho, .025)
up4 <- quantile(rho, .975)
rbind(c(lo=lo0, up=up0), c(lo=lo4, up=up4))
```
Alternatively, we can use the CLT and the "delta" method
to derive a asymptotic CI which yields:
```{r ex1.3}
lo1 <- t0 - qnorm(.975)*sd(x)/2/mean(x)/sqrt(n)
up1 <- t0 + qnorm(.975)*sd(x)/2/mean(x)/sqrt(n)
as.numeric(c(lo=lo1, up=up1))
```
We can compare it with the previous one
```{r ex1.4}
( rbind(c(lo=lo0, up=up0),
      c(lo=lo1, up=up1)) )
```
and note that the bootstrap one is shorter. 


### Example 2 - Estimate the expected value of the square root 

In this example, we have a sample $X_1, \ldots, X_n$ and are
interested in making inference on $\theta = E_F[ \sqrt{X} ]$. 

Here again we will generate the sample ourselves, so 
we will know the actual true value of $\theta$. 
We use  $X \sim \chi^2_1$, and thus $\sqrt{X} \sim |Z|$,
the absolute value of a standard normal 
random variable $Z \sim {\cal N}(0,1)$. Hence, 
$\theta = E[ |Z| ] = \sqrt{2/\pi} = 0.7978846$. 

As before, first we generate a sample: 
```{r ex2}
set.seed(123456)
n <- 30
x <- rchisq(n, df=1)
```
A natural estimator for $\theta$ is $(1/n) \sum_{i=1}^n \sqrt{X}_i$:
```{r ex2.1}
(t0 <- mean(sqrt(x)))
```
Similarly to what we did above, we use $M = 5000$ 
bootstrap samples to estimate the sampling distribution of 
$\hat{\theta}_n = (1/n) \sum_{i=1}^n \sqrt{X}_i$:
```{r ex2.2}
M <- 5000
rho <- vector('numeric', M)
for(j in 1:M) {
  bb <- sample.int(n, repl=TRUE)
  rho[j] <- mean(sqrt(x[bb]))
}
```
We now  build an
approximate 95\% confidence interval for $\theta$
using the estimated quantiles of the sampling distribution
of our estimator:
```{r ex2.3}
lo0 <- t0 - quantile(rho - t0, .975)
up0 <- t0 - quantile(rho - t0, .025)
lo1 <- quantile(rho, .025)
up1 <- quantile(rho, .975)
rbind(c(lo=lo0, up=up0), c(lo=lo1, up=up1))
```
<!-- Note that this CI barely contains the true value of the parameter.  -->
<!-- In fact, this is not surprising.  -->
<!-- We use a histogram and a boxplot of the recomputed -->
<!-- estimators as a visual "sanity check", and note -->
<!-- that the bootstrap distribution has a noticeable bias: -->
<!-- ```{r ex2.21, fig.show="hold", out.width="50%"} -->
<!-- hist(rho, main=expression(rho)); abline(v=sqrt(2/pi), lwd=3, col='steelblue') -->
<!-- boxplot(rho, main=expression(rho)) -->
<!-- abline(h=sqrt(2/pi), lwd=3, col='steelblue') -->
<!-- ``` -->

<!-- Bias-corrected confidence intervals attempt to fix this -->
<!-- problem, as we will discuss later in class.  -->
<!-- ```{r ex2.4} -->
<!-- (tmp <- mean(rho <= t0)) -->
<!-- (alph <- pnorm(qnorm(.025) + 2*qnorm(tmp))) -->
<!-- lo3 <- t0 - (quantile(rho, 1-alph) - t0) -->
<!-- up3 <- t0 - (quantile(rho, alph) - t0) -->
<!-- rbind(B=c(lo=lo0, up=up0), BC=c(lo=lo3, up=up3)) -->
<!-- ``` -->

Can you think of another method to build an
approximate CI for $E[ \sqrt{X} ]$? 


### Example 3 - The bootstrap's automatic transformation

This example illustrates an important property of the
simple bootstrap percentile confidence intervals.
Given a sample $X_1, \ldots, X_n$ we are interested
in $\theta = e^\mu$ where $\mu = E(X)$. 
First, lets  generate a small sample:
```{r ex3}
set.seed(123)
n <- 20
x <- rnorm(n)
```
The MLE estimator is $\hat{\theta}_n = e^{\bar{X}_n}$:
```{r ex3.1}
(t0 <- exp(mean(x)))
```
Use bootstrap to estimate quantiles of the
statistic of interest
```{r ex3.2}
M <- 5000
rho <- vector('numeric', M)
for(j in 1:M) {
  bb <- sample.int(n, repl=TRUE)
  rho[j] <- exp(mean(x[bb]))
}
```
Now use  the estimated quantiles to build an
approximate 95\% confidence interval:
```{r ex3.5}
lo1 <- quantile(rho, .025)
up1 <- quantile(rho, .975)
c(lo=lo1, up=up1)
```
Compare this with a CLT based CI:
```{r ex3.mle}
c(lo=t0 - qnorm(.975)*sd(rho), 
  up=t0 + qnorm(.975)*sd(rho))
```
Note that the distribution of our estimator
is quite asymmetric:
```{r ex3.31, fig.show="hold", out.width="50%"}
hist(rho, main=expression(rho))
boxplot(rho, main=expression(rho))
```

So it should not a surprise that these two confidence
intervals differ from each other. However, 
we now that if we transform our estimator with `log`,
then, the distribution should be quite symmetric,
and we expect both CIs (bootstrap and "normal")
to be closer to each other. 
Let us verify this in this example.  
```{r ex3.10}
t0 <- mean(x)
M <- 5000
rho <- vector('numeric', M)
for(j in 1:M) {
  bb <- sample.int(n, repl=TRUE)
  rho[j] <- mean(x[bb])
}
lo1.log <- quantile(rho, .025)
up1.log <- quantile(rho, .975)
lo2.log <- t0 - qnorm(.975)*sd(rho)
up2.log <- t0 + qnorm(.975)*sd(rho)
rbind(c(lo=lo1.log, up=up1.log),
      c(lo=lo2.log, up=up2.log))
```
Note that now the two CIs are fairly similar.
But also note that if we transform 
these back to the `exp` scale, 
they all resemble the one we obtained with 
the bootstrap percentile method:
```{r exp3.17}
rbind(BootLog=c(lo=exp(lo1.log), up=exp(up1.log)),
      GausLog=c(lo=exp(lo2.log), up=exp(up2.log)),
      Boot = c(lo=lo1, up=up1))
```


<!-- $X$ and $Y$: -->
<!-- ```{r ex3} -->
<!-- set.seed(123) -->
<!-- n <- 30 -->
<!-- la <- rchisq(n, df=1) -->
<!-- x <- rexp(n, rate=1) + la -->
<!-- y <- runif(n, min=2, max=3) + la -->
<!-- ``` -->
<!-- We are interested in $E[ X/Y ]$. Note that $P( Y > 0 ) = 1$.  -->
<!-- A natural point estimator is -->
<!-- ```{r ex3.1} -->
<!-- (t0 <- mean(x/y)) -->
<!-- ``` -->
<!-- Use bootstrap to estimate quantiles of the -->
<!-- statistic of interest -->
<!-- ```{r ex3.2} -->
<!-- M <- 5000 -->
<!-- rho <- vector('numeric', M) -->
<!-- for(j in 1:M) { -->
<!--   bb <- sample.int(n, repl=TRUE) -->
<!--   rho[j] <- mean(x[bb]/y[bb]) -->
<!-- } -->
<!-- ``` -->
<!-- ```{r ex3.21, fig.show="hold", out.width="50%"} -->
<!-- hist(rho, main=expression(rho)) -->
<!-- boxplot(rho, main=expression(rho)) -->
<!-- ``` -->

<!-- We can use $E^*[ X/Y ]$ as an estimator for -->
<!-- $E[ X/ Y]$ and thus an estimator of the bias -->
<!-- is -->
<!-- ```{r ex3.3} -->
<!-- mean(rho) - t0  -->
<!-- ``` -->
<!-- and an estimator of the variance of  -->
<!-- $(1/n) \sum_{i=1}^n X_i/Y_i$ -->
<!-- ```{r ex3.4} -->
<!-- var(rho) -->
<!-- ``` -->
<!-- We can now use the estimated quantiles to build an -->
<!-- approximate 95\% confidence interval: -->
<!-- ```{r ex3.5} -->
<!-- lo0 <- t0 - quantile(rho - t0, .975) -->
<!-- up0 <- t0 - quantile(rho - t0, .025) -->
<!-- as.numeric(c(lo=lo0, up=up0)) -->
<!-- ``` -->

<!-- These are simple quantile CI's.  -->

<!-- Better (bias corrected) quantile estimators: -->

<!-- ### Example 4 - CI for a correlation -->
<!-- Generate correlated $X$ and $Y$: -->
<!-- ```{r ex4.0} -->
<!-- set.seed(123) -->
<!-- n <- 100 -->
<!-- la <- rchisq(n, df=1) - 1 -->
<!-- x <- rexp(n, rate=1) + la -->
<!-- y <- runif(n, min=-1, max=1) + la -->
<!-- ``` -->
<!-- Now compute the bootstrapped estimators -->
<!-- ```{r ex4.1} -->
<!-- M <- 5000 -->
<!-- rho <- vector('numeric', M) -->
<!-- for(j in 1:M) { -->
<!--   bb <- sample.int(n, repl=TRUE) -->
<!--   rho[j] <- cor(x[bb], y[bb]) -->
<!-- } -->
<!-- ``` -->
<!-- Compute the quantiles -->
<!-- ```{r ex4.2} -->
<!-- t0 <- cor(x, y) -->
<!-- lo0 <- t0 - (quantile(rho, .975) - t0) -->
<!-- up0 <- t0 - (quantile(rho, .025) - t0) -->
<!-- ``` -->
<!-- The normal (transformed) approach: -->
<!-- ```{r ex4.3} -->
<!-- zz <- log((1+t0)/(1-t0))/2 -->
<!-- lo <- zz - qnorm(.975)/sqrt(n-3) -->
<!-- up <- zz + qnorm(.975)/sqrt(n-3) -->
<!-- lo2 <- (exp(2*lo)-1)/(exp(2*lo)+1) -->
<!-- up2 <- (exp(2*up)-1)/(exp(2*up)+1) -->
<!-- ``` -->
<!-- The bootstrap one isn't really better -->
<!-- ```{r ex4.4} -->
<!-- rbind(c(lo0, up0),  -->
<!--       c(lo2, up2)) -->
<!-- ``` -->

<!-- We can do better: -->
<!-- ```{r ex4.5} -->
<!-- (tmp <- mean(rho <= t0)) -->
<!-- (alph <- pnorm(qnorm(.025) + 2*qnorm(tmp))) -->
<!-- lo3 <- t0 - (quantile(rho, 1-alph) - t0) -->
<!-- up3 <- t0 - (quantile(rho, alph) - t0) -->
<!-- rbind(c(lo0, up0),  -->
<!--       c(lo2, up2), -->
<!--       c(lo3, up3)) -->
<!-- ``` -->


<!-- M <- 1000 -->
<!-- lev5 <- lev4 <- lev3 <- lev2 <- lev <- 0 -->
<!-- len5 <- len4 <- len3 <- len2 <- len <- 0 -->
<!-- ff <- function(a, x) 1/mean(x[a]) -->
<!-- n <- 10 -->
<!-- lambda0 <- 5 -->
<!-- a <- qnorm(.025) -->
<!-- b <- qnorm(.975) -->

<!-- for(j in 1:M) { -->

<!--   set.seed(123 + 17*j) -->

<!--   x <- rexp(n, rate=lambda0) -->

<!--   # Estimate quantiles -->
<!--   B <- 5000 -->
<!--   bootsam <- matrix(sample.int(n, size=B*n, replace=TRUE), B, n) -->

<!--   tstar <- apply(bootsam, 1, ff, x=x) -->

<!--   t0 <- 1/mean(x) -->

<!--   lo <- t0 - quantile(tstar, .975) + t0 -->
<!--   up <- t0 - quantile(tstar, .025) + t0 -->
<!--   if((lambda0 > lo ) & (lambda0 < up) ) lev <- lev + 1 -->

<!--   len <- len + (up - lo) -->

<!--   bootsam <- matrix(rexp(n*B, rate=1/mean(x)), B, n) -->

<!--   tstar <- apply(bootsam, 1, function(a) 1/mean(a)) -->

<!--   lo <- t0 - quantile(tstar, .975) + t0 -->
<!--   up <- t0 - quantile(tstar, .025) + t0 -->
<!--   if((lambda0 > lo ) & (lambda0 < up) ) lev2 <- lev2 + 1 -->
<!--   len2 <- len2 + (up - lo) -->


<!--   up <- t0*(1-a/sqrt(n)) -->
<!--   lo <- t0*(1-b/sqrt(n)) -->
<!--   if( (lambda0 < up ) & (lambda0 > lo ) ) lev3 <- lev3 + 1 -->
<!--   len3 <- len3 + (up - lo) -->

<!--   up <-t0*(1+b/sqrt(n)) -->
<!--   lo <- t0*(1+a/sqrt(n)) -->
<!--   if( (lambda0 < up ) & (lambda0 > lo ) ) lev4 <- lev4 + 1 -->
<!--   len4 <- len4 + (up - lo) -->

<!--   up <- (1/t0) - a*sd(x)/sqrt(n) -->
<!--   lo <- (1/t0) - b*sd(x)/sqrt(n) -->
<!--   if( (lambda0 < (1/lo) ) & (lambda0 > (1/up) ) ) lev5 <- lev5 + 1 -->
<!--   len5 <- len5 + (1/lo - 1/up) -->


<!--   # print(c(it=j, bnp=lev/j, bp=lev2/j, clt=lev3/j, clt2=lev4/j))   -->
<!--   print(c(it=j, bnp=len/j, bp=len2/j, clt=len3/j, clt2=len4/j, clt3=len5/j))   -->

<!-- } -->

<!-- print(c(bnp=lev/M, bp=lev2/M, clt=lev3/M, clt2=lev4/M, clt3=lev5/M))   -->
<!-- print(c(bnp=len/M, bp=len2/M, clt=len3/M, clt2=len4/M, clt3=len5/M))   -->



<!-- ### CI for a correlation -->

<!-- set.seed(123) -->
<!-- n <- 30 -->
<!-- la <- rchisq(n, df=1) - 1 -->
<!-- x <- rexp(n, rate=1) + la -->
<!-- y <- runif(n, min=-1, max=1) + la -->

<!-- M <- 5000 -->
<!-- rho <- vector('numeric', M) -->
<!-- for(j in 1:M) { -->
<!--   bb <- sample.int(n, repl=TRUE) -->
<!--   rho[j] <- cor(x[bb], y[bb]) -->
<!-- } -->

<!-- t0 <- cor(x, y) -->
<!-- lo0 <- t0 - (quantile(rho, .975) - t0) -->
<!-- up0 <- t0 - (quantile(rho, .025) - t0) -->


<!-- zz <- log((1+t0)/(1-t0))/2 -->
<!-- lo <- zz - qnorm(.975)/sqrt(n-3) -->
<!-- up <- zz + qnorm(.975)/sqrt(n-3) -->

<!-- lo2 <- (exp(2*lo)-1)/(exp(2*lo)+1) -->
<!-- up2 <- (exp(2*up)-1)/(exp(2*up)+1) -->

<!-- print(c(lo0, up0)) -->
<!-- print(c(lo2, up2)) -->



<!-- ## Mean of a ratio of dependent variables -->



<!-- set.seed(123) -->
<!-- n <- 30 -->
<!-- la <- rchisq(n, df=1) - 1 -->
<!-- x <- rexp(n, rate=1) + la -->
<!-- y <- runif(n, min=2, max=3) + la -->

<!-- t0 <- mean(x/y) -->

<!-- M <- 5000 -->
<!-- rho <- vector('numeric', M) -->
<!-- for(j in 1:M) { -->
<!--   bb <- sample.int(n, repl=TRUE) -->
<!--   rho[j] <- mean(x[bb]/y[bb]) -->
<!-- } -->

<!-- # bias? -->
<!-- mean(rho) - t0 -->

<!-- # variance -->
<!-- var(rho) -->


<!-- ## mean of sqrt(X^2_1) = |N(0,1)| -->
<!-- # sigma*sqrt(2/pi)*exp(-mu^2/(2 sigma^2)) + mu(1-2*pnorm(-mu/sigma)) -->
<!-- # mu=0, sigma=1 -->
<!-- # sqrt(2/pi) = 0.7978846 -->

<!-- set.seed(123) -->
<!-- n <- 30 -->
<!-- x <- rchisq(n, df=1) -->

<!-- t0 <- mean(sqrt(x)) -->

<!-- M <- 5000 -->
<!-- rho <- vector('numeric', M) -->
<!-- for(j in 1:M) { -->
<!--   bb <- sample.int(n, repl=TRUE) -->
<!--   rho[j] <- mean(sqrt(x[bb])) -->
<!-- } -->

<!-- lo0 <- t0 - (quantile(rho, .975) - t0) -->
<!-- up0 <- t0 - (quantile(rho, .025) - t0) -->
<!-- c(lo=lo0, up=up0) -->

<!-- # an alternative method? -->

<!-- ## sqrt(E(X)) -->

<!-- set.seed(123) -->
<!-- n <- 30 -->
<!-- x <- rexp(n, rate=4) -->

<!-- t0 <- sqrt(mean(x)) -->

<!-- M <- 5000 -->
<!-- rho <- vector('numeric', M) -->
<!-- for(j in 1:M) { -->
<!--   bb <- sample.int(n, repl=TRUE) -->
<!--   rho[j] <- sqrt(mean(x[bb])) -->
<!-- } -->

<!-- lo0 <- t0 - (quantile(rho, .975) - t0) -->
<!-- up0 <- t0 - (quantile(rho, .025) - t0) -->

<!-- lo1 <- t0 - qnorm(.975)*sd(x)/2/mean(x)/sqrt(n) -->
<!-- up1 <- t0 + qnorm(.975)*sd(x)/2/mean(x)/sqrt(n) -->
<!-- rbind(c(lo=lo0, up=up0), -->
<!--       c(lo=lo1, up=up1)) -->








