{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LICENSE\n",
    "These notes are released under the \n",
    "\"Creative Commons Attribution-ShareAlike 4.0 International\" license. \n",
    "See the **human-readable version** [here](https://creativecommons.org/licenses/by-sa/4.0/)\n",
    "and the **real thing** [here](https://creativecommons.org/licenses/by-sa/4.0/legalcode). \n",
    "\n",
    "# Introduction, goals, general setting\n",
    "\n",
    "Traditionally (i.e. 60 or\n",
    "70 years ago), statistical analyses (Data Science analyses? \n",
    "Learning methods?) started with a\n",
    "question of interest (e.g. is this fertilizer better than that one?\n",
    "does this drug reduce cholesterol levels?), \n",
    "and then data was collected, typically via \n",
    "an experiment (under somewhat controlled circumstances). More recently,\n",
    "however, the analyses often start from an existing data set. \n",
    "When Statistics (Data Science? Learning?) is involved, \n",
    "it is generally believed that there exists an unknown \n",
    "stochastic phenomenon that generated the data. A model for this\n",
    "process is postulated, which often (but not always) depends on\n",
    "a set of parameters. Queries about the model (including\n",
    "interpreting its components) are \n",
    "typically phrased in terms of questions about the parameters,\n",
    "and these are answered based on estimated values for them\n",
    "(think of confidence intervals, point estimators, etc.) \n",
    "When the interest is in constructing\n",
    "a predictor, the parameters have no interest in themselves, \n",
    "but they still need to be estimated in order to use the\n",
    "model with future data. \n",
    "\n",
    "Estimation methods are generally not unique. Historically\n",
    "they have been chosen based on their performance \n",
    "(e.g. statistical properties of the resulting estimators,\n",
    "computational cost or feasibility, etc.) Common\n",
    "criteria include accuracy (e.g. mean squared error: MSE) and prediction\n",
    "power (e.g. mean squared prediction error: MSPE). \n",
    "These criteria usually need themselves to \n",
    "be estimated, or computed, which requires making assumptions \n",
    "(e.g. that the data follow a certain family\n",
    "of distributions, that the model is a \"good enough\"\n",
    "approximation to the stochastic phenomenon generating\n",
    "the data, etc.) \n",
    "\n",
    "Once sufficient assumptions have been made, \"optimal\"\n",
    "estimation methods can be derived. The most famous \n",
    "example is the large class of Maximum Likelihood\n",
    "Estimators. \n",
    "\n",
    "**Question**: to what extent do these optimal \n",
    "method remain \"good enough\" if the assumptions\n",
    "are relaxed? **Answer**: none&mdash;they usually fail badly \n",
    "very quickly.\n",
    "\n",
    "**Question**: can we find estimation methods \n",
    "that remain \"good enough\" under weaker \n",
    "assumptions (a wider range of possibilities) for\n",
    "the stochastic phenomenon generating\n",
    "the data? **Answer**: Yes. This is what we do \n",
    "in Robust Statistics. \n",
    "\n",
    "In these notes, \"weaker assumptions\" will mean\n",
    "relaxing the assumptions on the distribution \n",
    "of the data, but we will still rely on the\n",
    "model being a \"good enough\"\n",
    "approximation to the process that generated\n",
    "the data. In this sense we talk about \n",
    "*Robust Statistics Given A Model*. \n",
    "\n",
    "One useful relaxation in practice \n",
    "is accepting that the data may not be\n",
    "*homogeneous*, i.e. that there may be either\n",
    "errors or observations that follow a different process. \n",
    "Other possible departures from the assumptions \n",
    "accompanying the model is that the \n",
    "distributions may belong to a similar but different family \n",
    "(e.g. that the data follow an elliptical\n",
    "distribution instead of Gaussian). \n",
    "Here we will mostly consider the former kind\n",
    "of departures, which have been called \"gross error\" model\n",
    "violations.\n",
    "\n",
    "In summary, we are interested in the problem of \n",
    "performing statistical analysis when the data may contain\n",
    "atypical observations. These atypical points may be \n",
    "\"errors\" (due to recording mistakes, equipment malfunction, etc.)\n",
    "or may be due to the presence of observations that follow\n",
    "a different stochastic phenomenon from the one generating\n",
    "the majority of\n",
    "the data. Generally our interest is the latter. Detecting\n",
    "(accurately identifying) potential \"outliers\" in the training\n",
    "set is also a common goal of Robust Statistics. Finally,\n",
    "we will also consider the problem of obtaining\n",
    "reliable predictions on future (\"good\") data when the \n",
    "training set may have atypical points. \n",
    "\n",
    "## A very personal \"agenda\" \n",
    "\n",
    "Rather than attempting a thorough, deep and exhaustive \n",
    "discussion of robust methods for a few \n",
    "simple models, in this notes I will try to illustrate \n",
    "the main current ideas and approaches as applied to \n",
    "models beyond univariate or multivariate location/scale \n",
    "and linear regression models.  \n",
    "\n",
    "For a detailed treatment of the basic concepts and\n",
    "techniques in Robust Statistics for the last 60 years, \n",
    "please refer to the following books:\n",
    "\n",
    "> Hampel, F. R., Ronchetti, E. M., Rousseeuw, P. J., Stahel, W. A., & Wiley Online Library. (2011;2005;). Robust statistics: The approach based on influence functions. Hoboken: Wiley. [UBC Library link](http://tinyurl.com/y2bnnzss)\n",
    "\n",
    "> Huber, P. J., Ronchetti, E., & Wiley Online Library. (2009). Robust statistics (2nd;2. Aufl.; ed.). Hoboken, N.J: Wiley. [UBC Library link](http://tinyurl.com/yxpyrsqq)\n",
    "\n",
    "> Maronna, R. A., Martin, R.D., Yohai, V.J. and Salibian-Barrera, M. Wiley Online Library. (2019). Robust statistics: Theory and methods (with R) (Second;2; ed.). Hoboken, NJ: WIley. doi:10.1002/9781119214656 [UBC Library link](http://tinyurl.com/yy4heaad)\n",
    "\n",
    "\n",
    "# Simple examples of robust estimators for linear regression\n",
    "\n",
    "\n",
    "### Simple linear regression\n",
    "We will use the `phosphor` data in \n",
    "package `RobStatTM` (for `R`). \n",
    "Details can be found using `help(phosphor, package='RobStatTM')`. \n",
    "The response variable is `plant` and, \n",
    "to simplify the example, we will use only one explanatory variable,\n",
    "`organic`. Furthermore, in order to \n",
    "highlight the potential impact of outliers, we will \n",
    "change the position of the single outlier in these data (from the \n",
    "right end of the plot to the left):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data(phosphor, package='robustbase')\n",
    "library(RobStatTM)\n",
    "# artificially change the location of the outlier \n",
    "# for illustration purposes\n",
    "phosphor[17, 'organic'] <- 15\n",
    "plot(plant ~ organic, data=phosphor, pch=19, col='gray50')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now fit a robust estimator for linear regression models \n",
    "(an MM-estimator) and overlay it in red on the above plot. \n",
    "The usual least squares estimator is shown in blue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MMfit <- lmrobdetMM(plant ~ organic, data=phosphor)\n",
    "LSfit <- lm(plant ~ organic, data=phosphor)\n",
    "plot(plant ~ organic, data=phosphor, pch=19, col='gray50')\n",
    "abline(MMfit, lwd=4, col='tomato3')\n",
    "abline(LSfit, lwd=4, col='steelblue3', lty=2)\n",
    "legend('topright', lwd=3, lty=c(1, 2), col=c('tomato3', 'steelblue3'), \n",
    "       legend=c('MM', 'LS'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- We look at the estimated regression coefficients: -->\n",
    "<!-- ```{r coef} -->\n",
    "<!-- cbind(MM=coef(MMfit), LS=coef(LSfit)) -->\n",
    "<!-- ``` -->\n",
    "Note that if outliers were not present in the data, then the \n",
    "robust and the least squares estimator coincide. \n",
    "The green line in the next plot corresponds to the OLS estimator\n",
    "computed without the outlier. Note that the robust fit \n",
    "is indistinguishable from this last one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(plant ~ organic, data=phosphor, pch=19, col='gray50')\n",
    "abline(MMfit, lwd=3, col='tomato3')\n",
    "abline(LSfit, lwd=3, col='steelblue3')\n",
    "LSclean <- lm(plant ~ organic, data=phosphor, subset=-17)\n",
    "abline(LSclean, lwd=3, col='green3')\n",
    "legend('topright', lwd=3, lty=1, col=c('tomato3', 'steelblue3', 'green3'), \n",
    "       legend=c('MM', 'LS', 'LS(clean)'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see that the estimated regression coefficients are\n",
    "in fact very close:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbind(MM=coef(MMfit), LS=coef(LSfit), LSclean=coef(LSclean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the MM-estimator is indistinguishable from the \n",
    "LS estimator computed on the clean data. This is the desired\n",
    "result of using an efficient and robust estimator. \n",
    "\n",
    "\n",
    "#### A synthetic toy example (diagnostics and estimation)\n",
    "\n",
    "This example will illustrate that:\n",
    "\n",
    "- outliers can be severerly damaging without being \"obviously\" deviating;\n",
    "- quantile regression estimators (L1) offer limited protection against atypical observations; and\n",
    "- classical diagnostic tools may not work as advertised.\n",
    "\n",
    "Our example contains $n = 200$ observations with $p = 6$\n",
    "explanatory variables. The regression model is $Y = \n",
    "V1 + 2*V2 + V3 + V4 + V5 + \\varepsilon$, where \n",
    "$\\varepsilon$ follows a $N(0, 1.7)$ distribution. \n",
    "Hence, the true vector of regression \n",
    "coefficients is `(1, 2, 1, 1, 1, 0)` and the true intercept is \n",
    "zero. The explanatory variables are all independent standard\n",
    "normal random variables. I used the following code \n",
    "to generate the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n <- 200\n",
    "p <- 6\n",
    "set.seed(123)\n",
    "x0 <- as.data.frame(matrix(rnorm(n*p), n, p))\n",
    "x0$y <- with(x0, V1 + 2*V2 + V3 + V4 + V5 + rnorm(n, sd=1.7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now replace the last 20 observations\n",
    "with outliers (for a total proportion of atypical observations of 20/200 = 10%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps <- .1\n",
    "n1 <- ceiling(n*(1-eps))\n",
    "x0[n1:n, 1:p] <- matrix(rnorm((n-n1+1)*p, mean=+1.85, sd=.8))\n",
    "x0$y[n1:n] <- rnorm(n-n1+1, mean=-7, sd=1.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These atypical observations cannot be seen easily in \n",
    "a pairwise plot, specially if one does not know \n",
    "in advance that they are present:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "message": "FALSE,",
     "warning": "FALSE"
    }
   },
   "outputs": [],
   "source": [
    "pairs(x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard diagnostic plots do not flag anything of \n",
    "importance either:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m0 <- lm(y~., data=x0)\n",
    "par(mfrow=c(2,2))\n",
    "plot(m0, which=c(1, 2, 5))\n",
    "par(mfrow=c(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that all the Cook distances are below 0.15, \n",
    "for example. \n",
    "However, the estimated regression coefficients are\n",
    "very different from the true ones\n",
    "(1, 2, 1, 1, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbind(LS=coef(m0), Truth=c(0,1, 2, 1, 1, 1, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now compare the estimated regression \n",
    "coefficients obtained with 2 other methods:\n",
    "an MM-estimator,\n",
    "and the L1-estimator (which is a quantile\n",
    "regression estimator). We will later see in\n",
    "the course that these estimators \n",
    "have different robustness properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(RobStatTM)\n",
    "myc <- lmrobdet.control(family='bisquare', efficiency=.85)\n",
    "m1 <- lmrobdetMM(y~., data=x0, control=myc)\n",
    "m3 <- quantreg::rq(y~., data=x0)\n",
    "cbind(Truth=c(0, 1, 2, 1, 1, 1, 0),\n",
    "      LS=coef(m0), L1=coef(m3), MM=coef(m1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compare the diagnostic plots obtained with\n",
    "the robust estimator, where the outliers are\n",
    "clearly visible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par(mfrow=c(2,2))\n",
    "plot(m1, which=c(1, 2, 4))\n",
    "par(mfrow=c(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add now the LS estimator computed on the \"clean\" data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m0.cl <- lm(y~., data=x0, subset = -(n1:n) )\n",
    "cbind(Truth=c(0, 1, 2, 1, 1, 1, 0),\n",
    "      LS=coef(m0), L1=coef(m3),  \n",
    "      MM=coef(m1), LSclean = coef(m0.cl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(m0)$coef"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.1.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
